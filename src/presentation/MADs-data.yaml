title: "Data Architecture Fundamentals: From Storage to Distributed Processing"
description: "Comprehensive 1.5-day workshop covering storage fundamentals, B+Trees, distributed processing with Spark, and advanced Parquet usage. Journey from single-machine limitations to petabyte-scale processing."
icon: "fas fa-network-wired"
category: "Data Architecture Workshop"
sections:
  - title: "Storage Fundamentals and File Access Patterns"
    slides:
      - type: "text"
        title: "Workshop Overview: Data at Scale"
        bullets:
          - "Day 1: Storage fundamentals → Distributed processing basics"
          - "Day 2: Delta-Lake → Transactional Distributed Storage over Object Stores"
          - "Journey from KB files to PB-scale distributed processing"
          - "Hands-on examples throughout the workshop"
        notes: "Set expectations for the comprehensive workshop covering the entire data processing stack."

      - type: "diagram"
        title: "Storage Stack: From Hardware to Applications"
        diagramRef: "storage_stack"
        bullets:
          - "Block Storage Devices: SSDs, HDDs, NVMe drives"
          - "OS Page Cache: 4KB pages bridging storage and memory"
          - "File Systems: ext4, NTFS, XFS managing block allocation"
          - "Applications: Databases, analytics engines, file processors"
        notes: "Establish the foundational storage hierarchy that everything builds upon."

      - type: "diagram"
        title: "File Structure: Collections of 4KB Pages"
        diagramRef: "file_pages"
        bullets:
          - "Files are logical collections of fixed-size pages (4KB each)"
          - "Page addressing: startOffset = pageId × pageSize"
          - "All I/O operations happen at page granularity"
          - "Foundation for understanding amplification effects"
        notes: "Show how files are structured as collections of pages, setting up amplification concepts."

      - type: "diagram"
        title: "Read Amplification: 100 Bytes → 4KB Page"
        diagramRef: "read_amplification"
        bullets:
          - "Application requests: 100 bytes (single record)"
          - "OS must read: Entire 4KB page (40 records)"
          - "40x amplification factor: Storage reads more than needed"
          - "Mitigated by: Page cache keeps page in memory for future reads"
        notes: "Show step-by-step how small reads trigger full page reads from storage."

      - type: "diagram"
        title: "Write Amplification: Read-Modify-Write Cycle"
        diagramRef: "write_amplification"
        bullets:
          - "Application updates: 10 bytes in existing record"
          - "OS must: Read 4KB page, modify 10 bytes, write 4KB page"
          - "400x amplification factor: 10 bytes → 8KB total I/O"
          - "Always expensive: Cannot be cached away like reads"
        notes: "Demonstrate why writes are always amplified and cannot be optimized like reads."

      - type: "diagram"
        title: "OLTP Pattern: Point Queries with Small Pages"
        diagramRef: "oltp_point_query"
        bullets:
          - "Point queries: 'Find customer ID 12345' → single record lookup"
          - "4-8KB pages minimize read amplification for small reads"
          - "Optimized for <10ms latency and high concurrency"
          - "Row-oriented storage keeps related data together"
        notes: "Show why OLTP uses small pages to minimize latency for point queries."

      - type: "diagram"
        title: "OLAP Pattern: Column Scans with Large Data Blocks"
        diagramRef: "olap_column_scan"
        bullets:
          - "Analytical queries: 'Sum all sales last month' → scan millions of rows"
          - "1-4MB stripes maximize bandwidth utilization"
          - "Column-oriented storage enables efficient compression and SIMD"
          - "Optimized for throughput, not latency"
        notes: "Demonstrate why OLAP uses large stripes to maximize scan performance."

      - type: "diagram"
        title: "Clustered B+Tree: Data Pages as Leaf Nodes"
        diagramRef: "clustered_btree_lookup"
        bullets:
          - "Single file stores entire B+tree with data at leaf level"
          - "Point lookup requires 3 random I/Os (root → internal → leaf)"
          - "Leaf pages contain full row data (no additional fetch needed)"
          - "Examples: InnoDB clustered tables, SQL Server clustered indexes"
        notes: "Show how clustered B+trees minimize I/O by storing data in leaf pages."

      - type: "diagram"
        title: "Heap + Index: Separate Data and Index Files"
        diagramRef: "heap_index_lookup"
        bullets:
          - "Index and data stored in separate files"
          - "Point lookup requires 4 random I/Os (3 index + 1 heap fetch)"
          - "Index leaf pages contain only {key, TID} pointers"
          - "Examples: PostgreSQL heap tables, Oracle non-clustered indexes"
        notes: "Demonstrate the additional I/O overhead when index and data are separated."

  - title: "B+Tree Deep Dive: Optimizing File Access"
    slides:
      - type: "text"
        title: "B+Tree Structure and Benefits"
        bullets:
          - "Balanced Tree: All leaves at same depth for consistent performance"
          - "Page-aligned Nodes: Each node fits in one OS page (4KB)"
          - "Fanout Factor: High branching reduces tree height"
          - "Sequential Leaf Access: Efficient for range queries"
        notes: "Explain the fundamental structure of B+Trees and why they're ideal for storage."

     # ─── Slide Cluster: Page-Aligned vs Non-Aligned B+Tree Nodes ───

      - type: "text"
        title: "B+Tree Node = OS Page: Perfect Alignment"
        bullets:
          - "Each B+Tree node = one OS page (4KB)"
          - "Single I/O loads complete node into page cache"
          - "~32-33 key+pointer pairs per 4KB node (80% fill factor)"
          - "High fanout (30-40) ⇒ tree height ≤ 3 for millions of rows"
          - "Matches CPU cache lines & flash blocks – no waste"
        notes: |
          Stress that classic B-trees were *co-designed* with 4 KB OS pages to minimise read & write amplification
          across disk, flash, and CPU caches. Mention fill-factor (70–90 %) so students appreciate real-world fan-out.
      
          **What is Fanout?**
          • Definition: Number of child pointers per internal node
          • Physical constraint: Limited by the 4KB page size
          
          **How is 30-40 calculated?**
          • 4KB node with 100-byte keys + 8-byte pointers = ~37 entries
          • Formula: (4096 - overhead) / (key_size + pointer_size)
          • Real example: (4096 - 100) / (100 + 8) ≈ 37 entries → fanout of ~38
          
          **Why does high fanout matter?**
          • Higher fanout = Shorter tree = Fewer I/O operations
          • Dramatic difference:
            - 1M records with fanout 40 = 3-level tree (3 I/Os max)
            - 1M records with fanout 2 = 20-level tree (20 I/Os max)
          
          Fanout is crucial to B+tree performance - it's the key multiplier that determines tree height and I/O efficiency.



      - type: "diagram"
        title: "Problems with Non-Aligned Nodes"
        diagramRef: "multiple_nodes_per_page"
        bullets:
          - "Thought-experiment: 1KB nodes → four nodes per 4KB page"
          - "Read Amplification: need 1KB, load 4KB (4× over-read)"
          - "Cache Pollution: 3KB unwanted data clogs caches"
          - "Latch Contention: four nodes share one page lock"
          - "Write Amplification: small update dirties entire 4KB"
        notes: |
          Visual demonstration of the amplification and cache pollution problems. Show how reading Node B 
          forces loading of unwanted Nodes A, C, and D.

      - type: "text"
        title: "Discussion: Could Memory-Mapped Files Help?"
        bullets:
          - "Scenario: keep 1KB nodes, use mmap() for 'selective' access"
          - "Hope: kernel lazily loads just the 1KB slice needed"
          - "Reality: page-fault still pulls full 4KB from disk"
          - "Readahead: kernel may prefetch unpredictably → bursty I/O"
          - "Good fit: append-only/read-only workloads (LMDB, DuckDB)"
          - "Question: when would you accept this trade-off?"
        notes: |
          Use this slide to spark discussion. Have students list mmap-based engines (LMDB, FoundationDB, DuckDB)
          and identify why they work (append-only, copy-on-write).

      - type: "diagram"
        title: "Why Memory Mapping *Doesn't* Solve Alignment"
        diagramRef: "aligned_vs_nonaligned_comparison"
        bullets:
          - "Read Amplification Remains: demand 1 KB → kernel issues 4 KB read (4x)"
          - "Dual Cache Pollution: unwanted 3 KB occupies OS page cache *and* CPU L2/L3"
          - "Latch Contention Multiplies: four logical nodes share one page-level lock"
          - "Loss of I/O Control: can't batch or pipeline with preadv()/async-io - kernel decides"
        notes: |
          Side-by-side comparison showing why page alignment is optimal. Even with memory mapping,
          the fundamental I/O and caching inefficiencies remain. Show 100% vs 25% efficiency.

      - type: "diagram"
        title: "B+Tree Example: Customer Database"
        diagramRef: "simple_btree_example"
        bullets:
          - "Scenario: 1 million customers, 100 bytes per record"
          - "File Size: 100MB total data"
          - "Without Index: Linear scan = 25,000 page reads average"
          - "With B+Tree: Point query = 3-4 page reads maximum"
          - "Height Calculation: log₁₀₀(1M) ≈ 3 levels + 1 leaf = 4 reads"
        notes: "Concrete example showing dramatic improvement in file access efficiency with visual B+Tree structure generated using Graphviz."

      - type: "text"
        title: "B+Tree Page Access Pattern"
        bullets:
          - "Root Page: Always in memory (most accessed)"
          - "Internal Pages: Cached based on query patterns"
          - "Leaf Pages: Contain actual data, loaded on demand"
          - "Range Scans: Follow leaf page links for sequential access"
        notes: "Show how B+Trees minimize actual storage device access through intelligent caching."

      - type: "diagram"
        title: "B+Tree Limitations at Scale"
        diagramRef: "database_scaling_bottlenecks"
        bullets:
          - "**500+ concurrent users** accessing scheduling data simultaneously"
          - "**60K+ IOPS requirements** for real-time operational systems"  
          - "**10TB+ per physical database** before horizontal partitioning becomes necessary"
          - "**Transaction ID wraparound** issues in production systems with billions of records"
        notes: "Show the architectural bottlenecks that force the transition from single-server to distributed systems."

      - type: "diagram"
        title: "Performance Cliff: Concurrent User Scaling"
        diagramRef: "btree_concurrent_users_limit"
        bullets:
          - "Linear performance up to ~500 concurrent users"
          - "Exponential degradation beyond the performance cliff"
          - "Lock contention and I/O saturation drive the breakdown"
          - "Real-world example: enterprise scheduling systems hitting this wall"
        notes: "Visual demonstration of how B+Tree systems hit hard performance limits with realistic concurrent user loads."

      - type: "diagram"
        title: "When B+Trees Aren't Enough: OLAP Challenges"
        diagramRef: "olap_analytics_architecture"
        bullets:
          - "**Analytics requirement**: Scan 17TB dataset in seconds, not hours"
          - "**Traditional approach**: 20+ minutes for complex analytical queries"
          - "**Modern columnar systems**: Same analysis completed in under 80 seconds"
          - "**15-20x improvement** needed for interactive analytics"
        notes: "Show the clear architectural challenge where row-oriented storage cannot deliver the performance needed for large-scale analytics."

      - type: "diagram"
        title: "Query Performance: The Analytics Gap"
        diagramRef: "olap_query_time_comparison"
        bullets:
          - "Traditional row storage scales exponentially with data size"
          - "Columnar storage maintains near-linear performance"
          - "Interactive threshold (2 minutes) shows business requirement"
          - "17TB example dataset demonstrates real-world scale"
        notes: "Quantify the performance gap between traditional and modern approaches, showing why architectural change is necessary."

  - title: "Introduction to Distributed Processing"
    slides:
      - type: "text"
        title: "Why Distributed Processing?"
        bullets:
          - "Data volumes exceed single machine capacity (TB to PB scale)"
          - "Processing time requirements demand parallel computation"
          - "Need for fault tolerance and high availability"
          - "Cost-effective scaling using commodity hardware"
        notes: "Start with the fundamental motivation for distributed processing. Emphasize that it's not just about big data, but also about performance, reliability, and cost."

      - type: "text"
        title: "Distributed Storage: Breaking the Single File Limit"
        bullets:
          - "File Chunking: Split large files into smaller blocks"
          - "Block Distribution: Store blocks across multiple machines"
          - "Replication: Multiple copies for fault tolerance"
          - "Example: 1TB file → 1000 × 1GB blocks across 100 machines"
        notes: "Show how distributed storage solves the fundamental single-machine limitation."

      - type: "text"
        title: "Challenges of Distributed Computing"
        bullets:
          - "Coordination: How do nodes work together efficiently?"
          - "Fault Tolerance: What happens when nodes fail?"
          - "Data Movement: How to minimize expensive network transfers?"
          - "Consistency: How to maintain correctness across nodes?"
        notes: "Highlight the core challenges that any distributed processing framework must solve."

      - type: "text"
        title: "HDFS: Distributed File System Foundation"
        bullets:
          - "Block Size: 128MB default (much larger than OS pages)"
          - "Replication: 3 copies for fault tolerance"
          - "NameNode: Tracks block locations (metadata server)"
          - "DataNodes: Store actual blocks (storage servers)"
        notes: "Introduce HDFS as the foundation for distributed storage, emphasizing larger block sizes."

      - type: "diagram"
        title: "Distributed Processing Architecture"
        diagramRef: "distributed_processing_architecture"
        bullets:
          - "Driver coordinates the overall execution"
          - "Workers execute tasks in parallel"
          - "Distributed storage provides data locality"
        notes: "Show the high-level architecture of distributed processing frameworks."

  - title: "Core Distributed Processing Concepts"
    slides:
      - type: "text"
        title: "RDD: Resilient Distributed Dataset"
        bullets:
          - "Distributed Collection: Data spread across cluster nodes"
          - "Immutable: Cannot be changed after creation"
          - "Fault Tolerant: Can recreate lost partitions"
          - "Lazy Evaluation: Computations deferred until action"
        notes: "Introduce RDD as the fundamental abstraction for distributed data processing."

      - type: "diagram"
        title: "RDD Structure and Partitioning"
        diagramRef: "rdd_partition_concept"
        bullets:
          - "Driver discovers HDFS blocks and creates partition mapping"
          - "Each partition corresponds to data that can be processed independently"
          - "Partition locations enable data locality optimization"
        notes: "Show how RDDs map to underlying storage blocks and enable parallel processing."

      - type: "text"
        title: "From HDFS Blocks to RDD Partitions"
        bullets:
          - "HDFS Block: 128MB physical storage unit"
          - "RDD Partition: Logical processing unit (maps to blocks)"
          - "1:1 Mapping: Each partition typically processes one block"
          - "Data Locality: Tasks scheduled on nodes with local data"
        notes: "Explain the critical relationship between storage blocks and processing partitions."

      - type: "diagram"
        title: "Partition to Task Assignment"
        diagramRef: "partition_to_task_assignment"
        bullets:
          - "Driver analyzes file structure and creates optimal partition strategy"
          - "Task scheduler assigns partitions to workers based on data locality"
          - "Each task processes exactly one partition independently"
        notes: "Demonstrate how the framework optimally assigns work to minimize data movement."

      - type: "text"
        title: "Task Scheduling and Execution"
        bullets:
          - "Job Submission: User triggers an action (collect, save)"
          - "Stage Creation: Framework analyzes dependencies"
          - "Task Generation: One task per partition per stage"
          - "Execution: Tasks run in parallel across cluster"
        notes: "Walk through the complete lifecycle from user action to parallel execution."

      - type: "diagram"
        title: "Task Scheduling Flow"
        diagramRef: "task_scheduling_flow"
        bullets:
          - "DAG Scheduler creates stages based on data dependencies"
          - "Task Scheduler optimizes placement using data locality"
          - "Executors run tasks and return results to driver"
        notes: "Show the complete flow of how work gets distributed and executed."

  - title: "Fault Tolerance in Distributed Systems"
    slides:
      - type: "text"
        title: "Why Fault Tolerance Matters"
        bullets:
          - "Scale Reality: 1000 nodes = multiple failures per day"
          - "Failure Types: Node crashes, network partitions, disk failures"
          - "User Expectation: Jobs should complete despite failures"
          - "Cost Impact: Restarting from beginning is expensive"
        notes: "Establish why fault tolerance is critical at scale and the types of failures to expect."

      - type: "diagram"
        title: "Fault Tolerance Scenarios"
        diagramRef: "fault_tolerance_scenarios"
        bullets:
          - "Task failure: Retry on different node"
          - "Node failure: Reschedule all tasks from that node"
          - "Data loss: Recompute using lineage information"
        notes: "Demonstrate different failure scenarios and how the framework handles each."

      - type: "text"
        title: "Lineage: The Key to Fault Tolerance"
        bullets:
          - "Lineage Graph: Track how each partition was computed"
          - "Recomputation: Recreate lost data instead of replication"
          - "Minimal Recovery: Only recompute affected partitions"
          - "Example: Lost partition recreated by re-reading source + transformations"
        notes: "Explain how lineage enables efficient fault tolerance without expensive replication."

      - type: "text"
        title: "Performance Optimization: Data Locality"
        bullets:
          - "NODE_LOCAL: Data on same machine as task (optimal)"
          - "RACK_LOCAL: Data on same rack as task (good)"
          - "ANY: Data on different rack (requires network transfer)"
          - "Performance Impact: 2-10x difference between local and remote"
        notes: "Show the dramatic performance impact of data locality and how frameworks optimize for it."

      - type: "diagram"
        title: "Data Locality Optimization"
        diagramRef: "data_locality_optimization"
        bullets:
          - "Scheduler prefers to run tasks where data is already stored"
          - "Fallback strategies when optimal placement isn't available"
          - "Network traffic minimization through intelligent scheduling"
        notes: "Demonstrate how the scheduler optimizes task placement to minimize network overhead."

  - title: "Introduction to Columnar Storage"
    slides:
      - type: "text"
        title: "Row vs Column Storage for Analytics"
        bullets:
          - "Row Storage: Traditional approach, one record per row"
          - "Column Storage: Group same attributes together"
          - "Analytics Advantage: Often need only few columns from wide tables"
          - "Example: 'SELECT sum(sales) FROM transactions' only needs sales column"
        notes: "Introduce the fundamental concept of columnar storage and its advantages for analytical workloads."

      - type: "text"
        title: "Why Columnar Storage for OLAP?"
        bullets:
          - "Column Pruning: Only read needed columns, ignore rest"
          - "Better Compression: Similar values compress better together"
          - "Vectorized Processing: CPU-efficient batch operations"
          - "Predicate Pushdown: Skip entire column chunks using statistics"
        notes: "Explain the specific benefits that make columnar storage ideal for analytical processing."

      - type: "text"
        title: "Apache Parquet: Columnar Format for Big Data"
        bullets:
          - "Open Source: Standard format across big data ecosystem"
          - "Optimized: Both storage efficiency and query performance"
          - "Framework Support: Spark, Flink, Presto, Drill compatibility"
          - "Schema Evolution: Handle changing data structures over time"
        notes: "Introduce Parquet as the leading columnar format for distributed processing."

  - title: "Object Store Fundamentals"
    slides:
      - type: "text"
        title: "Object Stores: Key-Value Nature"
        bullets:
          - "Flat namespace: No real directories, just key prefixes"
          - "Immutable objects: Write-once, read-many model"
          - "No append operations: Must rewrite entire object"
          - "No partial updates: Full object replacement required"
        notes: "Introduce the fundamental characteristics of object stores that make them different from traditional file systems."

      - type: "diagram"
        title: "Object Store: Key-Value Nature and Limitations"
        diagramRef: "objectstore_keyvalue"
        bullets:
          - "Keys are unique string identifiers (no hierarchy)"
          - "Values are immutable blobs (no partial updates)"
          - "No native support for transactions or atomic operations"
          - "Example: PUT/GET operations on S3-like storage"
        notes: "Visual representation of object store's key-value nature and its fundamental limitations."

      - type: "text"
        title: "Impact on Data Storage"
        bullets:
          - "Tables can't be single files: Need multiple objects"
          - "Updates require new versions: No in-place modifications"
          - "Consistency challenges: No native transaction support"
          - "Performance implications: Full object rewrites"
        notes: "Explain how object store limitations affect data storage and processing patterns."

  - title: "Parquet File Structure Deep Dive"
    slides:
      - type: "text"
        title: "Parquet Hierarchical Structure"
        bullets:
          - "File Level: Complete Parquet file with header and footer"
          - "Row Groups: ~128MB horizontal partitions for parallelism"
          - "Column Chunks: All values for one column within a row group"
          - "Pages: ~1MB units for I/O and compression"
        notes: "Explain the four-level hierarchy that enables both parallel processing and efficient I/O."

      - type: "text"
        title: "Row Group Example: Breaking Down a 1GB File"
        bullets:
          - "File Size: 1GB Parquet file"
          - "Row Groups: 8 groups × 128MB each"
          - "Columns: 7 columns per row group"
          - "Column Chunks: 56 total (8 row groups × 7 columns)"
          - "Pages: ~896 pages (8 × 7 × ~16 pages per chunk)"
        notes: "Provide concrete numbers to make the hierarchical structure tangible and understandable."

      - type: "diagram"
        title: "Parquet File Organization"
        diagramRef: "parquet_file_structure"
        bullets:
          - "Footer contains metadata about all row groups and columns"
          - "Row groups enable parallel processing"
          - "Column chunks enable column pruning"
          - "Pages enable efficient I/O and compression"
        notes: "Visualize how the Parquet structure enables both parallel processing and query optimization."

      - type: "text"
        title: "Parquet Metadata: The Secret to Efficiency"
        bullets:
          - "Footer Metadata: Complete schema and statistics"
          - "Row Group Stats: Min/max/null count per column per row group"
          - "Page Stats: Fine-grained statistics within row groups"
          - "Reading Process: Only 2 reads needed to get all metadata"
        notes: "Explain how Parquet's rich metadata enables aggressive query optimization."

      - type: "text"
        title: "Parquet Reading Process"
        bullets:
          - "Step 1: Read last 8 bytes (footer length + magic number)"
          - "Step 2: Read footer (~32KB) containing all metadata"
          - "Step 3: Use metadata to plan optimal read strategy"
          - "Step 4: Read only required row groups and columns"
        notes: "Walk through the efficient reading process that minimizes I/O operations."

  - title: "Distributed Processing with Parquet"
    slides:
      - type: "text"
        title: "Parquet + Distributed Processing: Perfect Match"
        bullets:
          - "Row Groups = Natural Partitions: Each row group becomes a task"
          - "Self-Describing: Metadata enables independent processing"
          - "Predicate Pushdown: Skip entire row groups using statistics"
          - "Column Pruning: Read only necessary data"
        notes: "Explain why Parquet's design aligns perfectly with distributed processing frameworks."

      - type: "diagram"
        title: "Parquet to RDD Mapping"
        diagramRef: "parquet_to_rdd_mapping"
        bullets:
          - "Driver reads Parquet footer to discover row groups"
          - "Each row group becomes one RDD partition"
          - "Tasks scheduled on nodes with local row group data"
          - "Parallel processing of independent row groups"
        notes: "Show how distributed frameworks map Parquet structure to parallel processing units."

      - type: "text"
        title: "Spark's Parquet Integration"
        bullets:
          - "ParquetInputFormat: Automatically creates one split per row group"
          - "Catalyst Optimizer: Pushes predicates down to Parquet reader"
          - "Vectorized Execution: Processes columnar data in batches"
          - "Data Locality: Schedules tasks on nodes with row group data"
        notes: "Detail how Apache Spark specifically optimizes for Parquet processing."

      - type: "diagram"
        title: "Complete Distributed Query Flow"
        diagramRef: "parquet_distributed_query"
        bullets:
          - "Query planning: Analyze predicates and required columns"
          - "Task creation: One task per qualifying row group"
          - "Parallel execution: Independent processing of row groups"
          - "Result collection: Combine results from all tasks"
        notes: "Demonstrate the complete flow of a distributed analytical query on Parquet data."

      - type: "text"
        title: "Query Optimization with Parquet"
        bullets:
          - "Predicate Pushdown: Skip row groups where max < filter value"
          - "Column Pruning: Only read columns used in query"
          - "Projection Pushdown: Apply SELECT early to reduce data movement"
          - "Example: Query 'SELECT name WHERE age > 30' reads only name/age columns from qualifying row groups"
        notes: "Show concrete examples of how query optimization works with Parquet metadata."

      - type: "diagram"
        title: "Data Locality in Parquet Processing"
        diagramRef: "parquet_data_locality"
        bullets:
          - "HDFS blocks contain Parquet row group data"
          - "Tasks scheduled on nodes with local row group blocks"
          - "Network traffic minimized through local processing"
          - "Fault tolerance through block replication"
        notes: "Demonstrate how data locality optimization works specifically with Parquet files."

  - title: "Advanced Performance Optimization"
    slides:
      - type: "text"
        title: "Memory Management and Vectorization"
        bullets:
          - "Vectorized Processing: Process columnar data in batches"
          - "Lazy Loading: Load column chunks only when needed"
          - "Dictionary Encoding: Reduce memory footprint for repeated values"
          - "Page-level Processing: Efficient memory utilization"
        notes: "Explain advanced techniques for optimizing memory usage and processing speed."

      - type: "text"
        title: "Compression and Encoding Strategies"
        bullets:
          - "Dictionary Encoding: Efficient for low-cardinality columns"
          - "Run-Length Encoding: Optimal for sorted or repeated data"
          - "Delta Encoding: Compress sequences of similar values"
          - "Compression Codecs: SNAPPY for speed, GZIP for size"
        notes: "Detail the various encoding and compression strategies available in Parquet."

      - type: "diagram"
        title: "End-to-End Spark + Parquet Execution"
        diagramRef: "spark_parquet_execution"
        bullets:
          - "Driver analyzes Parquet metadata and creates execution plan"
          - "Tasks distributed to executors with row group assignments"
          - "Parallel processing with predicate and projection pushdown"
          - "Results collected and combined for final output"
        notes: "Show the complete execution flow highlighting all optimization techniques."

      - type: "text"
        title: "Performance Tuning Guidelines"
        bullets:
          - "Row Group Size: Balance parallelism vs overhead (128MB optimal)"
          - "Partition Strategy: Align with common query patterns"
          - "Compression Selection: SNAPPY for compute-bound, GZIP for I/O-bound"
          - "Column Organization: Place frequently queried columns first"
        notes: "Provide practical guidance for optimizing Parquet performance in production."

      - type: "text"
        title: "Real-World Performance Impact"
        bullets:
          - "Query Performance: 10-100x faster than row-based formats"
          - "Storage Efficiency: 75% compression typical"
          - "Network Reduction: 90% less data movement through column pruning"
          - "Cost Savings: Significant reduction in compute and storage costs"
        notes: "Quantify the real-world benefits achievable with optimized Parquet + Spark processing."

  - title: "Workshop Summary and Next Steps"
    slides:
      - type: "text"
        title: "Journey Completed: From KB to PB"
        bullets:
          - "Started: Single-machine file access with B+Trees"
          - "Learned: Distributed processing fundamentals"
          - "Mastered: Parquet + Spark optimization techniques"
          - "Result: Can now handle petabyte-scale analytical workloads"
        notes: "Summarize the complete journey from traditional database concepts to modern big data processing."

      - type: "text"
        title: "Key Architectural Principles"
        bullets:
          - "Data Locality: Always prefer local processing over network transfer"
          - "Parallelism: Design for independent, parallel processing units"
          - "Metadata: Rich metadata enables aggressive optimization"
          - "Fault Tolerance: Build resilience into every layer"
        notes: "Highlight the core principles that drive effective data architecture design."

      - type: "text"
        title: "Next Steps for Implementation"
        bullets:
          - "Start Small: Begin with single-node optimization"
          - "Measure Everything: Profile before and after optimizations"
          - "Incremental Migration: Gradually move to distributed processing"
          - "Monitor Performance: Track locality ratios and compression effectiveness"
        notes: "Provide practical guidance for applying these concepts in real projects."

      - type: "text"
        title: "Additional Resources"
        bullets:
          - "Apache Spark Documentation: In-depth guides and best practices"
          - "Parquet Format Specification: Understanding file format details"
          - "Performance Tuning Guides: Framework-specific optimization tips"
          - "Community Forums: Join discussions and learn from others"
        notes: "Point to resources for continued learning and community support."
